{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prioritized Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import enviroment_no_visual as enviroment_no_visual\n",
    "import enviroment_visual as enviroment_visual\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzione per creare una neural network lineare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_QNet(units):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape=[11]))\n",
    "    model.add(keras.layers.Dense(units, activation='relu'))\n",
    "    model.add(keras.layers.Dense(units=3))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementazione di un buffer circolare che permetta inserimento/cancellazione degli elementi e accesso random veloce. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, max_size, zeta=0.6):\n",
    "        self.max_size = max_size\n",
    "        self.zeta = zeta\n",
    "        self.epsilon = 1e-6  # Piccolo valore per evitare divisioni per zero\n",
    "        self.buffer = []\n",
    "        self.priorities = []\n",
    "        self.position = 0\n",
    "\n",
    "    def add(self, error, experience):\n",
    "        priority = self.get_priority(error)\n",
    "        if len(self.buffer) < self.max_size:\n",
    "            self.buffer.append(experience)\n",
    "            self.priorities.append(priority)\n",
    "        else:\n",
    "            self.buffer[self.position] = experience\n",
    "            self.priorities[self.position] = priority\n",
    "        self.position = (self.position + 1) % self.max_size\n",
    "\n",
    "    def get_priority(self, error):\n",
    "        return (error + self.epsilon) ** self.zeta\n",
    "\n",
    "    def slice_experiences(samples):\n",
    "        states, actions, rewards, next_states, game_overs = [\n",
    "            np.array([experience[field_index] for experience in samples])\n",
    "                        for field_index in range(5)]\n",
    "        return states, actions, rewards, next_states, game_overs\n",
    "\n",
    "    def sample(self, batch_size, beta):\n",
    "        priorities = np.array(self.priorities)\n",
    "        probabilities = priorities / priorities.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        sampling_probabilities = probabilities[indices]\n",
    "        weights = np.power(len(self.buffer) * sampling_probabilities, -beta)\n",
    "        weights /= weights.max()  # Normalizzazione dei pesi\n",
    "\n",
    "        states, actions, rewards, next_states, game_overs = slice_experiences(samples)\n",
    "\n",
    "        return states, actions, rewards, next_states, game_overs, indices, weights\n",
    "\n",
    "    def update(self, indices, errors):\n",
    "        for i, error in zip(indices, errors):\n",
    "            self.priorities[i] = self._get_priority(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTrainer:\n",
    "    def __init__(self, model, lr, gamma):\n",
    "        self.online_model = model\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "        \n",
    "    @tf.function\n",
    "    def train_step(self, states, actions, rewards, next_states, dones, weights):\n",
    "        next_Q_values = self.model(next_states)\n",
    "        max_next_Q_values = tf.reduce_max(next_Q_values, axis=1)\n",
    "        # Equazione di Bellman: Q value = reward + discount factor * expected future reward\n",
    "        target_Q_values = rewards + (1 - dones) * self.gamma * max_next_Q_values\n",
    "        with tf.GradientTape() as tape:\n",
    "            all_Q_values = self.model(states)  \n",
    "            Q_values = tf.reduce_sum(all_Q_values * actions, axis=1, keepdims=True)\n",
    "            td_errors = target_Q_values - Q_values\n",
    "            loss = tf.reduce_mean(weights * tf.square(td_errors))\n",
    "        # Backpropagation\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        errors = np.abs(td_errors.numpy())\n",
    "        return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensorflow(states, actions, rewards, next_states, dones):\n",
    "    states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "    actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "    rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "    next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "    dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "    return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, lr, gamma, max_memory, batch_size, nn_model):\n",
    "        self.n_games = 0\n",
    "        self.epsilon = 1  \n",
    "        self.lr = lr\n",
    "        self.gamma = gamma  \n",
    "        self.memory = PrioritizedReplayBuffer(max_size=max_memory)\n",
    "        self.beta = 0.4\n",
    "        self.batch_size = batch_size\n",
    "        self.model = nn_model\n",
    "        self.target_model = keras.models.clone_model(self.model)\n",
    "        self.trainer = QTrainer(self.model, self.target_model, lr=self.lr, gamma=self.gamma)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(convert_to_tensorflow(state, action, reward, next_state, done))\n",
    "\n",
    "    def train_memory(self, beta):\n",
    "        states, actions, rewards, next_states, dones, indices, weights = self.memory.sample(self.batch_size, self.beta)\n",
    "        errors = self.trainer.train_step(states, actions, rewards, next_states, dones, weights)\n",
    "        self.replay_buffer.update(indices, errors)\n",
    "\n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(3)\n",
    "        else:\n",
    "            Q_values = self.model(state[np.newaxis])\n",
    "            return np.argmax(Q_values[0])\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        final_move = [0, 0, 0]  \n",
    "        move = self.epsilon_greedy_policy(state)\n",
    "        final_move[move] = 1\n",
    "        return final_move\n",
    "    \n",
    "    def train_agent(self, N_GAME, visual=True):\n",
    "        if visual:\n",
    "            env = enviroment_visual.SnakeGameAI(speed=0)\n",
    "        else:\n",
    "            env = enviroment_no_visual.SnakeGameAI()\n",
    "        score_list = []\n",
    "        record = 0\n",
    "        n_eps_zero = int(N_GAME*0.7)\n",
    "        step=0\n",
    "        while self.n_games < N_GAME:\n",
    "            state_old = env.get_state()\n",
    "            final_move = self.get_action(state_old)\n",
    "            state_new, reward, done, score = env.play_step(final_move)\n",
    "            self.remember(state_old, final_move, reward, state_new, done)\n",
    "\n",
    "            if done:\n",
    "                env.reset()\n",
    "                self.n_games += 1\n",
    "                print(f\"\\rGame: {self.n_games}, Epsilon: {self.epsilon:3f}, Score: {score}, Record: {record}, Step eseguiti: {step}.\", end=\"\")\n",
    "                self.epsilon = max(((n_eps_zero - self.n_games) / n_eps_zero), 0)\n",
    "                if len(self.memory.max_size) >= self.batch_size:\n",
    "                    self.train_memory(beta)\n",
    "                    beta = min(1.0, beta + (N_GAME - self.n_games)/N_GAME*0.6)\n",
    "                if score > record:\n",
    "                    record = score\n",
    "                    self.save_model()\n",
    "                score_list.append(score)\n",
    "            step+=1\n",
    "        return score_list\n",
    "\n",
    "    def save_model(self, model_dir_path=\"./DQNmodel\", file_name='model.keras'):\n",
    "        if not os.path.exists(model_dir_path):\n",
    "            print(f\"La cartella non esiste. Sar√† creata con nome: {model_dir_path}\")\n",
    "            os.mkdir(model_dir_path)\n",
    "        file_name = os.path.join(model_dir_path, file_name)\n",
    "        self.model.save(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostra andamento dello score per partita durante il training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trand(scores, save_path=None):\n",
    "    zeros = [0 for i in range(49)]\n",
    "    calcola_media = lambda i: sum(scores[i-50:i+1]) / 50\n",
    "    media_precedenti = zeros + list(map(calcola_media, range(49, len(scores))))\n",
    "    max_mean_value = max(media_precedenti)\n",
    "    max_mean_index = media_precedenti.index(max_mean_value)\n",
    "    plt.plot(scores, label='Score')\n",
    "    plt.plot(media_precedenti, label='Mean score delle ultime 50 partite')\n",
    "    plt.text(max_mean_index, max_mean_value, f'{max_mean_value:.2f}', fontsize=12, color=\"darkorange\", ha='center')\n",
    "\n",
    "    plt.title(\"Andamento del training.\")\n",
    "    plt.xlabel(\"Partite\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend()\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea e allena un agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PrioritizedReplayBuffer.add() missing 5 required positional arguments: 'state', 'action', 'reward', 'next_state', and 'done'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m online_model \u001b[38;5;241m=\u001b[39m Linear_QNet(\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m      2\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, max_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100_000\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, nn_model\u001b[38;5;241m=\u001b[39monline_model)\n\u001b[0;32m----> 3\u001b[0m plot_scores \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN_GAME\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisual\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m plot_trand(plot_scores)\n",
      "Cell \u001b[0;32mIn[30], line 48\u001b[0m, in \u001b[0;36mAgent.train_agent\u001b[0;34m(self, N_GAME, visual)\u001b[0m\n\u001b[1;32m     46\u001b[0m final_move \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_action(state_old)\n\u001b[1;32m     47\u001b[0m state_new, reward, done, score \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mplay_step(final_move)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremember\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_old\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_move\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m     51\u001b[0m     env\u001b[38;5;241m.\u001b[39mreset()\n",
      "Cell \u001b[0;32mIn[30], line 15\u001b[0m, in \u001b[0;36mAgent.remember\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremember\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action, reward, next_state, done):\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_to_tensorflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: PrioritizedReplayBuffer.add() missing 5 required positional arguments: 'state', 'action', 'reward', 'next_state', and 'done'"
     ]
    }
   ],
   "source": [
    "online_model = Linear_QNet(128)\n",
    "agent = Agent(lr=0.05, gamma=0.9, max_memory=100_000, batch_size=1024, nn_model=online_model)\n",
    "plot_scores = agent.train_agent(N_GAME=500, visual=False)\n",
    "plot_trand(plot_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SnakeRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
