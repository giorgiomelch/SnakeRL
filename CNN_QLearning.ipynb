{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import enviroment_no_visual as enviroment_no_visual\n",
    "import enviroment_visual as enviroment_visual\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementazione di un buffer circolare che permetta inserimento/cancellazione degli elementi e accesso random veloce. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, max_size, zeta=0.6):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = []\n",
    "        self.zeta = zeta\n",
    "        self.priorities = np.zeros((max_size,), dtype=np.float32)\n",
    "        self.index = 0\n",
    "\n",
    "    def append(self, experience):\n",
    "        max_priority = self.priorities.max() if self.buffer else 1.0\n",
    "        if len(self.buffer) < self.max_size:\n",
    "            self.buffer.append(experience)\n",
    "        else:\n",
    "            self.buffer[self.index] = experience\n",
    "        self.priorities[self.index] = max_priority\n",
    "        self.index = (self.index + 1) % self.max_size\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        if len(self.buffer) == self.max_size:\n",
    "            priorities = self.priorities\n",
    "        else:\n",
    "            priorities = self.priorities[:self.index]\n",
    "        \n",
    "        probabilities = priorities ** self.zeta\n",
    "        probabilities /= probabilities.sum()\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        total = len(self.buffer)\n",
    "        sampling_probabilities = probabilities[indices]\n",
    "        weights = (total * sampling_probabilities) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*samples))\n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "\n",
    "    def update_priorities(self, batch_indices, batch_errors):\n",
    "        for i, error in zip(batch_indices, batch_errors):\n",
    "            self.priorities[i] = np.abs(error) + 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzione per creare una neural network lineare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_DQN():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(21, 21, 1)),  \n",
    "        tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(200, activation='relu'),\n",
    "        tf.keras.layers.Dense(4, activation='linear')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6400</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">804</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_13 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_6 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_14 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_15 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6400\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │     \u001b[38;5;34m1,280,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m804\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,668,844</span> (6.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,668,844\u001b[0m (6.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,668,844</span> (6.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,668,844\u001b[0m (6.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = CNN_DQN()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork:\n",
    "    def __init__(self, lr, gamma):\n",
    "        self.online_model = self.load_model() #CNN_DQN()\n",
    "        self.target_model = keras.models.clone_model(self.online_model)\n",
    "        self.target_model.set_weights(self.online_model.get_weights())\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, states, actions, rewards, next_states, dones, weights):\n",
    "        next_Q_values = self.online_model(next_states)\n",
    "        # Double DQN: l'online model sceglie l'azione dei prossimi stati ma i Q-Value sono stimati da target_model\n",
    "        best_next_actions = tf.argmax(next_Q_values, axis=1)\n",
    "        mask_for_target = tf.one_hot(best_next_actions, 4)\n",
    "        max_next_Q_values = tf.reduce_sum(self.target_model(next_states) * mask_for_target, axis=1)\n",
    "        # Equazione di Bellman: Q value = reward + discount factor * expected future reward\n",
    "        target_Q_values = rewards + (1 - dones) * self.gamma * max_next_Q_values\n",
    "        with tf.GradientTape() as tape:\n",
    "            all_Q_values = self.online_model(states)  \n",
    "            Q_values = tf.reduce_sum(all_Q_values * actions, axis=1, keepdims=False)\n",
    "            loss = tf.reduce_mean(weights * self.loss_fn(target_Q_values, Q_values))\n",
    "        # Backpropagation\n",
    "        grads = tape.gradient(loss, self.online_model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.online_model.trainable_variables))\n",
    "        td_errors = tf.abs(tf.subtract(target_Q_values, Q_values))\n",
    "        return td_errors\n",
    "\n",
    "    def update_weights(self):\n",
    "        self.target_model.set_weights(self.online_model.get_weights())\n",
    "\n",
    "    def save_model(self, model_dir_path=\"./DQNmodel/CNN\", file_name='model.keras'):\n",
    "        if not os.path.exists(model_dir_path):\n",
    "            print(f\"La cartella non esiste. Sarà creata con nome: {model_dir_path}\")\n",
    "            os.mkdir(model_dir_path)\n",
    "        file_name = os.path.join(model_dir_path, file_name)\n",
    "        self.online_model.save(file_name)\n",
    "    \n",
    "    def load_model(self):\n",
    "        model = tf.keras.models.load_model(\"./DQNmodel/CNN/model.keras\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensorflow(states, actions, rewards, next_states, dones):\n",
    "    states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "    actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "    rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "    next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "    dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "    return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, lr, gamma, max_memory, batch_size, visual=True):\n",
    "        self.n_games = 0\n",
    "        self.tau = 1\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = PrioritizedReplayBuffer(max_size=max_memory, zeta=0.6)\n",
    "        self.dqnetwork = DQNetwork(lr=lr, gamma=gamma)\n",
    "        self.env = enviroment_visual.SnakeGameAI(speed=0) if visual else enviroment_no_visual.SnakeGameAI()\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append(convert_to_tensorflow(state, action, reward, next_state, done))\n",
    "\n",
    "    def train_memory(self, beta=0.4):\n",
    "        if len(self.memory.buffer) < self.batch_size:\n",
    "            return\n",
    "        states, actions, rewards, next_states, dones, indices, weights = self.memory.sample(self.batch_size, beta)\n",
    "        td_errors = self.dqnetwork.train_step(states, actions, rewards, next_states, dones, weights)\n",
    "        self.memory.update_priorities(indices, td_errors.numpy())\n",
    "\n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        if np.random.rand() < self.tau:\n",
    "            return np.random.randint(4)\n",
    "        else:\n",
    "            Q_values = self.dqnetwork.online_model(state[np.newaxis])\n",
    "            return np.argmax(Q_values[0])\n",
    "    \n",
    "    def softmax_policy(self, state):\n",
    "        Q_values = self.dqnetwork.online_model(state[np.newaxis])\n",
    "        if self.tau < 0.005: # per evitare buffer overflow\n",
    "            return np.argmax(Q_values[0])\n",
    "        else:\n",
    "            max_Q = np.max(Q_values)\n",
    "            exp_q = np.exp((Q_values - max_Q) / self.tau)\n",
    "            probabilities = exp_q / np.sum(exp_q)\n",
    "            action = np.random.choice(4, p=probabilities[0])\n",
    "            return action\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        final_move = [0, 0, 0, 0]  \n",
    "        move = self.softmax_policy(state)\n",
    "        final_move[move] = 1\n",
    "        return final_move\n",
    "    \n",
    "    def train_agent(self, N_GAME):\n",
    "        score_list = []\n",
    "        record = 0\n",
    "        step=0 \n",
    "        n_tau_zero = int(N_GAME*0.7)\n",
    "        while self.n_games < N_GAME:\n",
    "            state_old = self.env.get_matrix_state()\n",
    "            final_move = self.get_action(state_old)\n",
    "            state_new, reward, done, score = self.env.play_step_m(final_move)\n",
    "            self.remember(state_old, final_move, reward, state_new, done)\n",
    "            if done:\n",
    "                self.env.reset()\n",
    "                self.n_games += 1\n",
    "                self.train_memory()\n",
    "                print(f\"\\rGame: {self.n_games}, Tau: {self.tau:3f}, Score: {score}, Record: {record}, Step eseguiti: {step}. \", end=\"\")\n",
    "                self.tau = max(((n_tau_zero - self.n_games) / n_tau_zero), 0)\n",
    "                if score > record:\n",
    "                    record = score\n",
    "                    self.dqnetwork.save_model()\n",
    "                if self.n_games % 10:\n",
    "                    self.dqnetwork.update_weights()\n",
    "                score_list.append(score)\n",
    "            step+=1\n",
    "        return score_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostra andamento dello score per partita durante il training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trend(scores, save_path=None):\n",
    "    calcola_media = lambda i: sum(scores[i-50:i]) / 50\n",
    "    media_precedenti = np.array([calcola_media(i) for i in range(50, len(scores) + 1)])\n",
    "    max_mean_value = np.max(media_precedenti)\n",
    "    max_mean_index = np.argmax(media_precedenti) + 50  \n",
    "    plt.plot(scores, label='Score')\n",
    "    plt.plot(range(50, len(scores) + 1), media_precedenti, label='Mean score delle ultime 50 partite')\n",
    "    plt.text(max_mean_index, max_mean_value, f'{max_mean_value:.2f}', fontsize=12, color=\"darkorange\", ha='center')\n",
    "    plt.title(\"Andamento del training\")\n",
    "    plt.xlabel(\"Partite\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0, 60)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea e allena un agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game: 10, Tau: 0.997429, Score: 0, Record: 0, Step eseguiti: 29. "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m, max_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30_000\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, visual\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m training_result \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN_GAME\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5_000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m plot_trend(training_result)\n",
      "Cell \u001b[0;32mIn[25], line 51\u001b[0m, in \u001b[0;36mAgent.train_agent\u001b[0;34m(self, N_GAME)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_games \u001b[38;5;241m<\u001b[39m N_GAME:\n\u001b[1;32m     50\u001b[0m     state_old \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mget_matrix_state()\n\u001b[0;32m---> 51\u001b[0m     final_move \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_old\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     state_new, reward, done, score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mplay_step_m(final_move)\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremember(state_old, final_move, reward, state_new, done)\n",
      "Cell \u001b[0;32mIn[25], line 40\u001b[0m, in \u001b[0;36mAgent.get_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m     39\u001b[0m     final_move \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]  \n\u001b[0;32m---> 40\u001b[0m     move \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     final_move[move] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_move\n",
      "Cell \u001b[0;32mIn[25], line 28\u001b[0m, in \u001b[0;36mAgent.softmax_policy\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoftmax_policy\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m---> 28\u001b[0m     Q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdqnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monline_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.005\u001b[39m: \u001b[38;5;66;03m# per evitare buffer overflow\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(Q_values[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/SnakeRL/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/SnakeRL/lib/python3.12/site-packages/keras/src/layers/layer.py:816\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    809\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly input tensors may be passed as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    810\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositional arguments. The following argument value \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    811\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould be passed as a keyword argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    812\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(arg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    813\u001b[0m             )\n\u001b[1;32m    815\u001b[0m \u001b[38;5;66;03m# Caches info about `call()` signature, args, kwargs.\u001b[39;00m\n\u001b[0;32m--> 816\u001b[0m call_spec \u001b[38;5;241m=\u001b[39m \u001b[43mCallSpec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_signature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m############################################\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;66;03m# 3. Check input spec for 1st positional arg.\u001b[39;00m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;66;03m# TODO: consider extending this to all args and kwargs.\u001b[39;00m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assert_input_compatibility(call_spec\u001b[38;5;241m.\u001b[39mfirst_arg)\n",
      "File \u001b[0;32m~/anaconda3/envs/SnakeRL/lib/python3.12/site-packages/keras/src/layers/layer.py:1574\u001b[0m, in \u001b[0;36mCallSpec.__init__\u001b[0;34m(self, signature, args, kwargs)\u001b[0m\n\u001b[1;32m   1572\u001b[0m     bound_args \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1573\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1574\u001b[0m     bound_args \u001b[38;5;241m=\u001b[39m \u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_arguments_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1576\u001b[0m     k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m bound_args\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1577\u001b[0m }\n\u001b[1;32m   1578\u001b[0m bound_args\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/anaconda3/envs/SnakeRL/lib/python3.12/inspect.py:3267\u001b[0m, in \u001b[0;36mSignature.bind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3263\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[1;32m   3264\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[1;32m   3265\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[1;32m   3266\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/SnakeRL/lib/python3.12/inspect.py\u001b[0m, in \u001b[0;36mSignature._bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = Agent(lr=0.001, gamma=0.99, max_memory=30_000, batch_size=1024, visual=False)\n",
    "training_result = agent.train_agent(N_GAME=5_000)\n",
    "plot_trend(training_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SnakeRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
