{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import enviroment\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgiomelch/anaconda3/envs/SnakeRL/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "input_shape = [11]\n",
    "n_outputs = 3\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(256, activation='relu', input_shape = input_shape),\n",
    "    keras.layers.Dense(n_outputs)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)\n",
    "    else:\n",
    "        Q_values = model.predict(state[np.newaxis], verbose=0)\n",
    "        return np.argmax(Q_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_buffer = deque(maxlen=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
    "    batch = [replay_buffer[index] for index in indices]\n",
    "    states, actions, rewards, next_states, game_over = [\n",
    "        np.array([experience[field_index] for experience in batch])\n",
    "                 for field_index in range(5)]\n",
    "    return states, actions, rewards, next_states, game_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "\n",
    "    final_move = [0,0,0]\n",
    "    final_move[action] = 1\n",
    "    \n",
    "    next_state, reward, game_over, score = env.play_step(final_move)\n",
    "    replay_buffer.append((state, action, reward, next_state, game_over))\n",
    "    return next_state, reward, game_over, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5000\n",
    "discount_factor = 0.9\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "def training_step(batch_size):\n",
    "    # Campionamento delle esperienze\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    # Predizione dei valori Q per gli stati successivi\n",
    "    next_Q_values = model.predict(next_states, verbose=0)\n",
    "    max_next_Q_values = next_Q_values.max(axis=1)\n",
    "    # Calcolo dei target utilizzando l'equazione di Bellman\n",
    "    runs = 1.0 - dones \n",
    "    target_Q_values = rewards + runs * discount_factor * max_next_Q_values\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    # Calcolo della loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    # Calcolo dei gradienti e aggiornamento del modello\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Episode: 1, Steps: 43, eps: 0.800, score: 0.0001\n",
      "Episode: 2, Steps: 23, eps: 0.790, score: 0.0002\n",
      "Episode: 3, Steps: 34, eps: 0.780, score: 0.0003\n",
      "Episode: 4, Steps: 51, eps: 0.770, score: 0.0004\n",
      "Episode: 5, Steps: 93, eps: 0.760, score: 0.0005\n",
      "Episode: 6, Steps: 27, eps: 0.750, score: 0.0006\n",
      "Episode: 7, Steps: 43, eps: 0.740, score: 0.0007\n",
      "Episode: 8, Steps: 72, eps: 0.730, score: 0.0008\n",
      "Episode: 9, Steps: 22, eps: 0.720, score: 0.0009\n",
      "Episode: 10, Steps: 75, eps: 0.710, score: 0.00010\n",
      "Episode: 11, Steps: 79, eps: 0.700, score: 0.00011\n",
      "Episode: 12, Steps: 15, eps: 0.690, score: 1.00012\n",
      "Episode: 13, Steps: 71, eps: 0.680, score: 0.00013\n",
      "Episode: 14, Steps: 44, eps: 0.670, score: 0.00014\n",
      "Episode: 15, Steps: 129, eps: 0.660, score: 0.00015\n",
      "Episode: 16, Steps: 64, eps: 0.650, score: 0.00016\n",
      "Episode: 17, Steps: 90, eps: 0.640, score: 0.00017\n",
      "Episode: 18, Steps: 62, eps: 0.630, score: 0.00018\n",
      "Episode: 19, Steps: 28, eps: 0.620, score: 0.00019\n",
      "Episode: 20, Steps: 25, eps: 0.610, score: 0.00020\n",
      "Episode: 21, Steps: 120, eps: 0.600, score: 1.00021\n",
      "Episode: 22, Steps: 55, eps: 0.590, score: 1.00022\n",
      "Episode: 23, Steps: 43, eps: 0.580, score: 0.00023\n",
      "Episode: 24, Steps: 26, eps: 0.570, score: 0.00024\n",
      "Episode: 25, Steps: 17, eps: 0.560, score: 0.00025\n",
      "Episode: 26, Steps: 147, eps: 0.550, score: 0.00026\n",
      "Episode: 27, Steps: 141, eps: 0.540, score: 0.00027\n",
      "Episode: 28, Steps: 92, eps: 0.530, score: 0.00028\n",
      "Episode: 29, Steps: 45, eps: 0.520, score: 0.00029\n",
      "Episode: 30, Steps: 165, eps: 0.510, score: 0.00030\n",
      "Episode: 31, Steps: 50, eps: 0.500, score: 0.00031\n",
      "Episode: 32, Steps: 63, eps: 0.490, score: 1.00032\n",
      "Episode: 33, Steps: 81, eps: 0.480, score: 1.00033\n",
      "Episode: 34, Steps: 188, eps: 0.470, score: 0.00034\n",
      "Episode: 35, Steps: 91, eps: 0.460, score: 0.00035\n",
      "Episode: 36, Steps: 80, eps: 0.450, score: 1.00036\n",
      "Episode: 37, Steps: 35, eps: 0.440, score: 0.00037\n",
      "Episode: 38, Steps: 196, eps: 0.430, score: 0.00038\n",
      "Episode: 39, Steps: 28, eps: 0.420, score: 0.00039\n",
      "Episode: 40, Steps: 32, eps: 0.410, score: 1.00040\n",
      "Episode: 41, Steps: 41, eps: 0.400, score: 0.00041\n",
      "Episode: 42, Steps: 106, eps: 0.390, score: 1.00042\n",
      "Episode: 43, Steps: 176, eps: 0.380, score: 0.00043\n",
      "Episode: 44, Steps: 32, eps: 0.370, score: 0.00044\n",
      "Episode: 45, Steps: 44, eps: 0.360, score: 0.00045\n",
      "Episode: 46, Steps: 74, eps: 0.350, score: 0.00046\n",
      "Episode: 47, Steps: 13, eps: 0.340, score: 1.00047\n",
      "Episode: 48, Steps: 127, eps: 0.330, score: 1.00048\n",
      "Episode: 49, Steps: 110, eps: 0.320, score: 2.00049\n",
      "Episode: 50, Steps: 30, eps: 0.310, score: 1.00050\n",
      "Episode: 51, Steps: 78, eps: 0.300, score: 1.00051\n",
      "Episode: 52, Steps: 17, eps: 0.290, score: 1.00052\n",
      "Episode: 53, Steps: 32, eps: 0.280, score: 0.00053\n",
      "Episode: 54, Steps: 158, eps: 0.270, score: 0.00054\n",
      "Episode: 55, Steps: 96, eps: 0.260, score: 0.00055\n",
      "Episode: 56, Steps: 26, eps: 0.250, score: 0.00056\n",
      "Episode: 57, Steps: 91, eps: 0.240, score: 1.00057\n",
      "Episode: 58, Steps: 101, eps: 0.230, score: 0.00058\n",
      "Episode: 59, Steps: 202, eps: 0.220, score: 1.00059\n",
      "Episode: 60, Steps: 47, eps: 0.210, score: 0.00060\n",
      "Episode: 61, Steps: 29, eps: 0.200, score: 1.00061\n",
      "Episode: 62, Steps: 207, eps: 0.190, score: 0.00062\n",
      "Episode: 63, Steps: 153, eps: 0.180, score: 1.00063\n",
      "Episode: 64, Steps: 133, eps: 0.170, score: 0.00064\n",
      "Episode: 65, Steps: 46, eps: 0.160, score: 0.00065\n",
      "Episode: 66, Steps: 337, eps: 0.150, score: 1.00066\n",
      "Episode: 67, Steps: 20, eps: 0.140, score: 0.00067\n",
      "Episode: 68, Steps: 101, eps: 0.130, score: 1.00068\n",
      "Episode: 69, Steps: 153, eps: 0.120, score: 1.00069\n",
      "Episode: 70, Steps: 37, eps: 0.110, score: 1.00070\n",
      "Episode: 71, Steps: 402, eps: 0.100, score: 0.00071\n",
      "Episode: 72, Steps: 57, eps: 0.090, score: 0.00072\n",
      "Episode: 73, Steps: 171, eps: 0.080, score: 1.00073\n",
      "Episode: 74, Steps: 402, eps: 0.070, score: 0.00074\n",
      "Episode: 75, Steps: 228, eps: 0.060, score: 0.00075\n",
      "Episode: 76, Steps: 301, eps: 0.050, score: 0.00076\n",
      "Episode: 77, Steps: 156, eps: 0.040, score: 1.00077\n",
      "Episode: 78, Steps: 301, eps: 0.030, score: 0.00078\n",
      "Episode: 79, Steps: 51, eps: 0.020, score: 1.00079\n",
      "Episode: 80, Steps: 402, eps: 0.010, score: 0.00080\n",
      "Episode: 81, Steps: 402, eps: 0.000, score: 0.00081\n",
      "Episode: 82, Steps: 106, eps: -0.010, score: 2.00082\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[157], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m restart:\n\u001b[1;32m     13\u001b[0m     epsilon \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m80\u001b[39m \u001b[38;5;241m-\u001b[39m episode) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m---> 14\u001b[0m     state, reward, done, score \u001b[38;5;241m=\u001b[39m \u001b[43mplay_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     restart \u001b[38;5;241m=\u001b[39m done\n\u001b[1;32m     17\u001b[0m     step\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[155], line 2\u001b[0m, in \u001b[0;36mplay_one_step\u001b[0;34m(env, state, epsilon)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplay_one_step\u001b[39m(env, state, epsilon):\n\u001b[0;32m----> 2\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mepsilon_greedy_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     final_move \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m     final_move[action] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[152], line 5\u001b[0m, in \u001b[0;36mepsilon_greedy_policy\u001b[0;34m(state, epsilon)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(n_outputs)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     Q_values \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(Q_values[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/SnakeRL/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/SnakeRL/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:509\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    507\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m--> 509\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menumerate_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_predict_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/SnakeRL/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:689\u001b[0m, in \u001b[0;36mTFEpochIterator.enumerate_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 689\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distributed_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches:\n\u001b[1;32m    691\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\n\u001b[1;32m    692\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution\n\u001b[1;32m    693\u001b[0m         ):\n",
      "File \u001b[0;32m~/anaconda3/envs/SnakeRL/lib/python3.12/site-packages/tensorflow/python/data/ops/dataset_ops.py:501\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[1;32m    500\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    503\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/SnakeRL/lib/python3.12/site-packages/tensorflow/python/data/ops/iterator_ops.py:709\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    705\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 709\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/SnakeRL/lib/python3.12/site-packages/tensorflow/python/data/ops/iterator_ops.py:748\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    745\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m    746\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[1;32m    747\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[0;32m--> 748\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/SnakeRL/lib/python3.12/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3478\u001b[0m, in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   3477\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3478\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3479\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3481\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rewards = [] \n",
    "best_score = 0\n",
    "env = enviroment.SnakeGameAI()\n",
    "MAX_N_GAMES = 200\n",
    "\n",
    "for episode in range(MAX_N_GAMES):\n",
    "    print(episode)\n",
    "    env.reset()    \n",
    "    state = env.get_state()\n",
    "    restart = False\n",
    "    step = 0\n",
    "    while not restart:\n",
    "        epsilon = (80 - episode) / 100\n",
    "        state, reward, done, score = play_one_step(env, state, epsilon)\n",
    "        \n",
    "        restart = done\n",
    "        step+=1\n",
    "    # extra code – displays debug info, stores data for the next figure, and\n",
    "    #              keeps track of the best model weights so far\n",
    "    print(f\"\\rEpisode: {episode + 1}, Steps: {step + 1}, eps: {epsilon:.3f}, score: {env.score}\",\n",
    "          end=\"\")\n",
    "    rewards.append(step)\n",
    "    if score >= best_score:\n",
    "        best_weights = model.get_weights()\n",
    "        best_score = score\n",
    "\n",
    "    training_step(batch_size)\n",
    "\n",
    "model.set_weights(best_weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SnakeRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
